---
title: "[論文輪読] A Deobfuscation Pre-Training Objective for ..."
emoji: "📜"
type: "tech"
topics: ["MachineLearning", "transcompiler"]
published: true
---

# A Deobfuscation Pre-Training Objective for Programming Languages

- [論文](https://arxiv.org/pdf/2102.07492.pdf)
- Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec, Guillaume Lample
- Paris Dauphine University, Facebook AI Research
- 2021/2/16 publish

# 概要

- ソースコードの難読化解除を教師なし学習で学習させたモデルで実現した


# キーワード

- MLM: Masked Language Modeling

# 1. Introduction

コード難読化は人間が理解しにくくするためであったり、機能を保ったままサイズを小さくするために行われる。現代においては主にコードの隠蔽化やマルウェア検出から逃れるためであったり、JavaScriptなどのコードを圧縮して通信量を削減するために行われる。Cのコードを例とすると変数名の変更などの不可逆な変換も行われる。（コンパイラ・デコンパイラで変数名が失われることも書かれていて、意図しているところがよくわかってない）

難読化されたファイルの全体の構造を解読するとき、経験豊富なプログラマは時間をかけてコードの意味を解析していく。論文内の例では幅優先探索アルゴリズムを例にとっていて、関数を認識してその形から細部を理解していくとしていて、このタスクはパターン認識に優れたニューラルネットワークで行うことができると書いてある。

そこで、この論文でsequence to sequenceモデルを訓練して難読化された関数を元の形に戻すモデルを訓練することを提案する。適切な変数名・関数名を復元するのはプログラムが何をしているのかを解析するよりも難しい作業で、MLMで事前学習を行う場合には構文だけでなくこのような変数名・関数名もマスクしてしまう（明示的には書かれていないが、おそらく学習をする意味がないものとして除外したいというニュアンスかと考えられる）。この論文で提案するモデルはこのような問題がなく、変数・関数を全て`F0`, `F1`, `V1`, `V2`といったスペシャルトークンで置き換える。

この論文のcontributionは

- DOBFという複数言語に対応した難読化解除器を提案
- DOBFがcode search, code summarization, unsupervised code translationといったタスクでMLMよりも優れていることを示す
- DOBFのモデルが完全に難読化されたソースファイルを難読化解除できることを示す


# 2. Related work

割愛

# 3. Model

## 3.1. MLM for Programming Languages

多くの文系でプリトレーングが紹介されている。ほとんどがハイパーパラメータと恣意的な決定に依存している（個々のトークンをマスクすべきかどうか、どの部分をマスクするかなど）。これは自然言語処理では経験的に行われていて検証が進んでいる。プログラミング言語は自然言語よりもはるかに構造化されているので、トークンの予測は自然言語のそれよりもはるかに容易になっている。

図1ではPythonのキーワードや構文を置き換えている。`[`, `while`, `=`, `if`, `)`, `return`などのシンボルは回復がしやすく、完全な正確さで予測することができ、モデルもすぐに学習する。この効果は言語の冗長性によって強調される。

文脈の把握に関してはMLMはプログラミング言語では単純すぎる（？）

## 3.2. Deobfuscation Objective

TODO: 書き換え。内容がDOBFのやることとなっているが、正確にはそれぞれへの入出力に関する説明。

DOBFはMLMよりもプログラミング言語の構造に特化させたもの。クラス名・関数名・変数名は特殊なトークンで置き換えることでコードスニペットを難読化して元の名前を復元するモデルをトレーニングする。識別子が選択されると、コード内の全てのインスタンスが同じトークンで置き換えられる。

MLMとの相違点として、トークンで置き換える際に同じ変数名は同じトークンで置き換えられる。MLMでは同じようにマスクされるわけではないので、同じ変数名でもマスクされる箇所とされない箇所がある。例えば図1の場合、`node`はDOBFでは`V5`で置き換えられているのに対して、MLMではマスクされている`[`と置き換えられていない`[`がある。

DOBFでは情報量の少ない構文関連のトークンはマスクされない。

識別子は確率$P_{obf}\in[0,1]$で置換される（ただし、かならず1つ以上置換されるようにする）。つまり、$P_{obf}=0$のときには識別子1つが置換され、$P_{obf}=1$のときにはファイル内全ての識別子が置換される。DOBFではコードの予約語や演算子に対しては置換処理を行わないので、置換を行ってもコードの実行結果は同じになる。（図1は幅優先探索のコードを$P_{obf}=1$で置換している）

良い結果を得るためにはモデルがコードの意味を深く学習している必要があり、プリトレーニングでやっていることの意味である。MLMのインプットではランダムにマスクをされているので、マスクのされ方によってはコードの本質を理解していなくてもマスク元が推測できる（例えば図1のMLMの４行目ではqueueが呼ばれているが、これを元に3行目のマスクがqueueであることが容易に推測される）。

## 3.3. Implementation

Deobfuscation Objectiveは機械翻訳のように働き、seq2seqモデルが学習される。（言いたいことはわかるがどう翻訳すべきか・・・・）

> Overall, the deobfuscation objective operates like a supervised machine translation objective, where a seq2seq model is trained to map an obfuscated code into a dictionary represented as a sequence of tokens. 

このモデルは推論時に難読化された識別子を持つコードに対して意味のあるクラス名・関数名・変数名を提案することができる。難読化されたクラス・関数・変数はCLASS_0...CLASS_N, FUNC_O...FUNC_N, VAR_0...VAR_Nのように置き換えられる。出力である辞書ではデリミタ記号`|`で各エントリが区切られる。

# 4. Experiments

## 4.1. Deobfuscation

$P_{obf}=0$と$P_{obf}=1$の2パターンで難読化解除タスクを行いモデルを評価する。

#### $P_{obf}=0$の場合

この場合では全体で一つの識別子のみ難読化されている。モデルは残りのコードをコンテクストとして難読化された識別子に対する意味のある名前を提案しなければならない。（PyCharmやIntelliJがシンプルなルールを使ってこのタスクを行える）

#### $P_{obf}=1$の場合

一般的に使われる難読化器はたいてい複数の変形を行う。そのうちの一つとして、全ての識別子の名前を短くて意味のない名前（例えばa, b, c）に置き換える。$P_{obf}=1$の置換はこの変形に相当する。モデルの性能を測るために、この変形を施したコードをモデルで元に戻させる。

#### 評価基準

モデルの評価には難読化前の識別子を用いる。元の識別子との完全マッチを真値として復元に成功した割合（パーセンテージ）で評価を行う。
また、先行研究にならって、サブトークンスコアも用いる。（サブトークンスコアとはより柔軟な評価軸で、大文字小文字を区別しないサブトークンを検索するための精度・リコール・F1スコア。。。。とあるが、よくわかってない。）

TODO: サブトークンスコアの詳細調査

それぞれのトークンはアッパーケースのキャメルケースとアンダースコアのスネークケースに分解される。例えば、`decoderAttention`の場合には`decoder_attention`と`attentionDecoder`にも完全一致する（順番変わるのはありなのか・・・・？）。`attention`であれば精度は高いがrecallは0.5なので、F1スコアは66.7となる。`crossAttentionDecoder`であれば精度は2/3で、F1スコアは80.0となる。

サブトークンの正確さ・recall・F1スコアの平均は全ての復元されたトークンに対して計算する。

## 4.2. 各タスク向けFine-tuning

プリトレーニングモデルとしてのDOBFを評価するため、DOBFをTransCoderとCodeXGLUEの3つのタスク向けにファインチューニングする。ここではJavaとPythonのみを対象として行う。

#### CodeXGLUE Clone Detection

2つのコードスニペットが意味的に等価であるかどうかをモデルが予測する二値分類問題。評価にはF1スコアを用いる。このモデルは、単一のエンコーダとclassificationレイヤで構成されている。入力は2つのコードスニペットで、モデルに与える前にそれらを連結する。このタスクはJavaで行う。

#### CodeXGLUE Code Summarization

モデルはコードスニペットが与えられるとそれに対応するドキュメントを自然言語で生成するように学習されている。アーキテクチャはBLEUスコアで評価されたseq2seqモデル。（よくわからん）データセットにはJavaとPythonが用いられている。

#### CodeXGLUE NL Code Search

自然言語によるコード検索クエリが与えられると、モデルはコードスニペットのコレクションの中から最も意味的に合致するコードを検索する。これはMean Reciprocal Rank(MRR)という指標を用いて評価されるランキング問題。モデルは2つのエンコーダで構成されている。自然言語のクエリとコードは別々にエンコードされて、エンコーダの最終段の第一隠れ層の状態のドット積を計算する（よくわからん）。このタスクはPythonで利用できる。

#### TransCoder

[前回の輪読で紹介した論文](https://zenn.dev/ten_takano/articles/20201007-ml-paper)で開発されたトランスコンパイラ。
We only consider a single model output (CA@1), with beam sizes of 1 and 10

## 4.3. 実験の詳細

### Model Architecture

アテンションメカニズム有りのseq2seqモデルを使用する。このモデルはトランスフォーマーアーキテクチャを用いたエンコーダとデコーダで構成されている（このあたりの関係がよくわかってない。この論文以前の問題かな）
基準（CodeBERT, TransCoder）との公平な比較を行うために異なる2つのモデルをトレーニングする。

- 12層、12のアテンションヘッド、768の隠れ次元
- 6層、8のアテンションヘッド、1024の隠れ次元

### Traning dataset

- GoogleのBigQueryでGithubからpublicなコードを持ってくる
- 対象はPython・Javaで書かれたコード
- 重複しているものは削除
- 各ファイルを難読化して、識別子を得る
    - Python：19GB
    - Java：26GB

### Traning details

DOBFは難読化されたファイルを識別子のリストに変換するように学習する。

- GPUごとに3000のトークンからなるJava・Pythonのバッチを交互に使用する
- [Adam optimizer](https://arxiv.org/abs/1412.6980)と[逆平方根学習率スケジューラ](https://arxiv.org/abs/1706.03762)を用いてDOBFを最適化する（詳細は調べてない）
- モデルはPyTorchで実装
- 32台のV100GPUで学習
- 学習を高速化するためにfloat16命令を仕様
- 初期化は異なる2つの方法で行う
    - 0から学習
    - Python-Java MLMによる学習
- DOBFを3つの異なる難読化確率（0, 0.5, 1）で学習させる
    - 検証データセットで計算されたサブトークンのF1スコアの平均値を用いて最適なものを選択する。

### Fine-tuning details

エンコーダとデコーダを備えたseq2seqモデル、2つのエンコーダを備えたモデル、1つのエンコーダを備えたモデルなど、目的に応じて様々なモデルアーキテクチャを検討する。比較を公平に行うため、元の論文と同じアーキテクチャ、GPU数、バッチサイズ、オプティマイザでモデルを学習。

CodeXGLUEでは、ファインチューニングの際に使用した学習率パラメータにタスクが非常に敏感に反応するめ、5x10^-6〜10^-4までの5つの学習率パラメータでグリッド検索を行い、検証データセットで最適なパラメータを選択。

TransCoderでは元論文と同様の10^-4の学習率を使用している。

# 実験結果

## 5.1. Deobfuscation

テーブル2
$P_{obf}=0$の評価を行う場合でも、$P_{obf}=0$で学習すると、各入力シーケンスに対して単一の変数のみ生成するようにしかモデルが学習しないので、$P_{obf}=0.5$よりも（学習？）効率が悪くなってしまう。ただし、$P_{obf}=0.5$での学習はモデルにコードの意味も理解させるより難易度の高いタスクなので、学習が難しい。したがって$P_{obf}=0$で学習する場合でもコードの構造学習という点においては優れているので有効。
DOBFが最もよく復元できたのは$P_{obf}=1$のときで、45.6%であった。また、全ての場合においてDOBF with MLMのプリトレーニングはパフォーマンスを向上させた。

図2はDOBFで関数を完全にリカバリーした時の図。DOBFは関数の目的を理解して入力元に近しい変数名を予測している。図3はDOBFがPythonで書かれた行列操作する関数の実装に対して変数名を提案している例。

DOBFは、キーとなるトークンを識別し、似ているようで全く異なる機能の目的を適切に推測することができることがわかる。

Appendixの図5,6,7にもJava/Python向けのDOBFによる変数提案の例を示されている。

Appendixの図8は難読化されていない識別子の意味を理解している。（？）

Appendixの図9と10は完全に難読化されたPythonのコードスニペットをDOBFを用いて解除した例。難読化されていたクラス・関数の目的と意味が理解できるようになっている。

## 5.2. Downstream tasks

ファインチューニングするにあたり、プリトレーニングでは$P_{obf}=0.5$と$P_{obf}=1$では非常に似た結果を得たため、以降$P_{obf}=0.5$のみ取り上げる。ベースラインとしてランダムに初期化されたモデルとMLMのみでプリトレーニングしたモデルを用いる。また、CodeXGLUEタスクについてはCodeBERTもベースラインとして用いる。

DOBFのみでトレーニングしたモデルとMLMを初期値としてDOBFでトレーニングしたモデルの比較をテーブル3に示す。ランダムに初期化したモデルは与えられたタスクに対してどれだけプリトレーニングが影響を与えるかを測る指標となる。プリトレーニングは特にNLCSタスクにおいて重要であった。MMRが0.025から0.383までMLMのプリトレーニングにより向上している。

MLMのベースラインとCodeBERTの最も大きな違いは、CodeBERTは

- CodeBERTは異なるデータセットでトレーンングされていること
- 追加のRTDオブジェクトを使用していること
- RoBERTaモデルで初期化されていること

コード要約とNLコード検索は自然言語を含むため、コメントを含むCodeBERTのデータセットの恩恵をウケる可能性があるが、このタスクではより単純なデータセットを使用して非常に近い結果が得られた。しかし、クローン検出においてはそのパフォーマンスに及ばなかった。また、RoBERTaでMLMモデルを初期化することも試みたが、downstreamタスクのパフォーマンスに大きな影響を与えることはなかった。

スクラッチでトレーニングしたDOBFとMLM+DOBFはいずれもdownstreamタスク全てに対して到達水準（CodeBERTとMLM）を満たす能力を獲得した。難読化解除では事前学習タスクとしてすでに効果的で、ほとんどのタスクでMLMと同等の結果をもたらし、クローンの検出ではより効果的である。MLM+DOBFのモデルは全てのdownstreamタスクに対してMLMを凌ぐ性能を獲得している。特にNLコード検索で大きな改善が見られた。

TransCoderに対しては、MLM+DOBFによってビームサイズ10のPythonからJavaへの変換で2.7%、JavaからPythonへの変換で5.9%、MLMモデルの計算精度を向上させた。

図4では、テストセットの計算精度はEpoc数にかかわらずMLM+DOBFのほうが高いことが示されている。また、コード検索やコード要約においてもMLM+DOBFはCodeBERTに大差をつけており、これらのタスクで効果的なモデルを学習するために自然言語に沿ったプログラミング言語データ（多分コメント有りのコードの意味）は必要ないことがわかる。

MLM+DOBFではほとんどのタスクでDOBFとMLMの両方よりも高いスコアが得られていて、MLMとDOBFが補完関係にあることがわかる。

# 6. 結論

新しい難読化解除器(DOBF)を提案した。DOBFは難読化されたコードの回復・関連する識別子の提案・プログラミング元g関連タスクの変換モデルの事前学習という3つの目的に使用できることが示された。DOBFは自然言語に頼ることなく能力を獲得できていて、クローン検出・コード要約・自然言語コード検索・教師なしコード翻訳といったタスクにおいて、CodeBERT・MLMの事前学習よりも優れている。

(ここからは翻訳したけどよくわからん)
これらの結果は、DOBFがソースコードの特殊な構造を利用して、特に効果的な方法で入力シーケンスにノイズを加えることを示しています。ソースコードに適合した他のノイズ関数やサロゲート目標があれば、さらに性能が向上する可能性があります。例えば、与えられた変数の型、メソッドのシグネチャ、破損したコードの修復などをモデルに学習させることができます。ソースコードで事前学習したモデルは、構造化されたノイズの恩恵を受けているので、この知見を自然言語にも適用できるかどうかは興味深いところである。自然言語は曖昧ではあるが、その根底には構造が存在している。プログラミング言語の抽象的な構文木ではなく、文の構成要素や依存関係の構文木を活用することで、自然言語の前処理目的をよりよく設計できるかもしれません。

# 参考

- [日本語の解説記事](https://ai-scholar.tech/articles/deep-learning/TransCoder)
    - 既に他の方が日本語解説記事を執筆されていました．評価周りはこちらの方が詳しい&正確そう．
- [arXivTimes](https://github.com/arXivTimes/arXivTimes)
    - こんなものがあるんですねっていうだけのリンク（今回の論文と直接関係は無い）
